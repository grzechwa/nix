*** tutaj będzie link do tworzenia/rozszerzenia FS na VMware (wink) ***
# lvcreate –L 3GB –n aldon nazwa_vg
# mkfs.ext4 /dev/mapper/nazwa_vg-aldon
# mkdir /aldon
# vi /etc/fstab
# mount -a


Powinniśmy dostać informację do jakiego serwera dystrybucyjnego powinna łączyć się aplikacja. Przykład: BSKTEST 
# telnet BSKTEST 5078
# nmap –p 5078 BSKTEST


Powinniśmy dostać informację jaki jest port nasłuchu. Default: 5079 
# lsof –i | grep 5079
# netstat –tulpn | grep 5079


# crontab –l
# crontab –e
0 0 * * 0 /aldon/LIBRPGM/czysc_LM.sh

BladeLogic

cgroups

cntlm

control-m

# cat /opt/ctmagent/installed-versions.txt

useradd -d /opt/ctmagent -m -s /bin/csh -u 3005 -g controlm ctmuser 
chmod 775 /opt/ctmagent 
chown ctmuser:controlm /opt/ctmagent

Control-M Installation


cd /opt/ctmagent 
tar xvfz DRKAI.7.0.00.Linux-i386.tar 
setup.sh -silent /opt/ctmagent/CTM7_Agent_Linux_Silent.xml


# fdisk -l
# pvcreate /dev/sdb
# pvs
# pvdisplay

# vgcreate VG_GROUP44 /dev/sdb
# vgs
# vgdisplay

# lvcreate -L512M -n LV_44 VG_GROUP44
# lvs
# lvs VG_GROUP44
# lvdisplay

# mkfs.ext4 /dev/mapper/VG_GROUP44-LV_44
# mkdir /mnt/LV_44
# vi /etc/fstab
/dev/mapper/VG_GROUP44-LV_44    /mnt/LV44    ext4    defaults    0 0

# mount -a
LVM Create

- Physical volume (PV) - is a partition configured to the LVM partiotion type
- Physical extent (PE) - is a small uniform segment of disk space. PVs are split into PEs
- Logical extent (LE) - is associated with a PE and can be combined into a volume group
- Volume Group (VG) - is a bunch of LEs grouped together
- Logical volume (LV) - is a part of a VG, which can be formatted and then mounted on the directory of your choice

Example:
1. We want to create LVM on disk /dev/sdb
2. VG name: VG_GROUP44
3. LV name: LV_44 
4. LV size: 512MB
4. Filesystem: ext4
5. Mounted on /mnt/LV_44

Create a Physical Volume

# pvcreate /dev/sdb
# pvs
# pvdisplay
*** if there is more than one partition to be configured as a PV, the associated device files can all be listed in the same command:
*** # pvcreate /dev/sda1 /dev/sda2 /dev/sdb1 /dev/sdb2

Create a Volume Group
*** From one or more PVs you can create a volume group (VG)
# vgcreate VG_GROUP44 /dev/sdb

# vgs
# vgdisplay


Create a Logical Volume

# lvcreate -L512M -n LV_44 VG_GROUP44
# lvs
# lvs VG_GROUP44
# lvdisplay

*** # lvcreate -l number_of_PEs volumegroup -n logvol
*** This creates a device named /dev/vloumegroup/logvol.
*** You can format this device as if it were a regula disk partition, and then mount a directory on that new loical volume.
*** But this isn't useful if you don't know how much space is associated with each PE.
*** Alteranatively, you can use the -L switch to set a size in MB. For example, the following command creates an LV named flex of 200MB:
*** # lvcreate -L 200M volumegroup -n flex
*** Example:
*** # lvcreate -l100% -n test01lv nazwa_grupy         |
*** # lvcreate -l+100%FREE -n test01lv nazwa_grupy

Format LV, create directory and mount
# mkfs.ext4 /dev/mapper/VG_GROUP44-LV_44
# mkdir /mnt/LV_44
# vi /etc/fstab
/dev/mapper/VG_GROUP44-LV_44    /mnt/LV44    ext4    defaults    0 0
# mount -a

VG Extend
Resize LV

# lvresize -L+2G (rozszerzenie o +...)
# lvresize -L2G (ustawienie dokładnie)

# lvresize -L2G /dev/mapper/nazwa_grupy-test01lv
# df -h /mnt/test01lv
# resize2fs /dev/mapper/nazwa_grupy-test01lv
# df -h /mnt/test01lv
# lvresize -L+512M /dev/mapper/nazwa_grupy-test01lv
# resize2fs /dev/mapper/nazwa_grupy-test01lv


# umount /dev/mapper/nazwa_grupy-test01lv
# fsck /dev/mapp..
# resize2fs /dev/mapp... 2G
# e2fsck -f /dev/mapp...
# resize2fs /dev/mapp... 2G
# lvresize -L2G /dev...

When we are increase disk - VMWare (in our case 21GB -> 30GB) changes can be invisible, than

# blockdev --getsize64 /dev/sdc
21...
# blockdev -rereadpt /dev/sdc
32...

# pvresize /dev/sdc
# pvs
# lvcreate -l+100%FREE -n maxlv testvg
# pvs
# pvcreate /dev/sdd
# vgextend testvg /dev/sdd

EXAMPLES


Example 1:

Please increase filesystems:
/javadump
/logwas
WebSphere

df -h
------------------------------------------------------
BEFORE
------------------------------------------------------
/dev/mapper/systemvg-javadump
                      9.9G  151M  9.2G   2% /javadump
/dev/mapper/systemvg-logwas
                       20G  6.9G   12G  37% /logwas
/dev/mapper/systemvg-websphere
                       14G  6.8G  6.4G  52% /WebSphere

-------------------------------------------------------
systemvg

# pvs | grep systemvg

Po rozmiarze szukamy na VMWare
rozszerzamy (w naszym przypadku o 54GB)
--------------------------------------------------
lepszym rozwiązaniem (pewnym) jest sprawdzenie w /sys/class/scsi_disk/.../device, 

example:
chcemy powiększyć /dev/sdk

ls -la /sys/class/scsi_disk/0\:0\:
0:0:0:0/  0:0:10:0/ 0:0:12:0/ 0:0:14:0/ 0:0:2:0/  0:0:4:0/  0:0:6:0/  0:0:9:0/
0:0:1:0/  0:0:11:0/ 0:0:13:0/ 0:0:15:0/ 0:0:3:0/  0:0:5:0/  0:0:8:0/

ls -la /sys/class/scsi_disk/0\:0\:11\:0/device/
block:sdk/            generic/              power/                scsi_disk:0:0:11:0/   type

wówczas na VMWare szukamy w Virtual Device Node
SCSI (0:11)
----------------------------------------------------


# blockdev --rereadpt /dev/sdb
# pvresize /dev/sdb
# pvs
# lvextend -r -L+10G /dev/mapper/systemvg-websphere
# lvextend -r -L+20G /dev/mapper/systemvg-logwas
# lvextend -r -L+24G /dev/mapper/systemvg-javadump
# df -h

EXAMPLE 2


proszę o dodanie 3GB dla /eformu/oradata

# df -h

/dev/mapper/eformuvgd0-oradatalv
                       29G   28G  1.2G  96% /eformu/oradata

eformuvgd0

# pvs | grep eformuvgd0
/dev/sde   eformuvgd0 lvm2 a--  29.00G    0

Po rozmiarze szukamy na VMWare
rozszerzamy (w naszym przypadku o 3GB)
--------------------------------------------------
lepszym rozwiązaniem (pewnym) jest sprawdzenie w /sys/class/scsi_disk/.../device, ex:
ls -la /sys/class/scsi_disk/0\:0\:
0:0:0:0/  0:0:10:0/ 0:0:12:0/ 0:0:14:0/ 0:0:2:0/  0:0:4:0/  0:0:6:0/  0:0:9:0/
0:0:1:0/  0:0:11:0/ 0:0:13:0/ 0:0:15:0/ 0:0:3:0/  0:0:5:0/  0:0:8:0/

ls -la /sys/class/scsi_disk/0\:0\:11\:0/device/
block:sdk/            generic/              power/                scsi_disk:0:0:11:0/   type
----------------------------------------------------

# blockdev --rereadpt /dev/sde
# pvresize /dev/sde
# lvextend -r -L+3G /dev/mapper/eformuvgd0-oradatalv
# df -h



 

EXAMPLE 3


Usunięcie FS /import

1. df -h
/dev/mapper/importvg-importlv
                       99G   20G   74G  21% /import


2. Sprawdzamy co jest w katalogu / jeżeli dane nie są już potrzebne
# umount /import

3. 
 remove z fstab

4.
# lvs
  LV                         VG       Attr      LSize   Pool Origin Data%  Move Log Cpy%Sync Convert
  importlv                   importvg -wi-ao--- 100.00g



# lvdisplay
  --- Logical volume ---
  LV Path                /dev/importvg/importlv
  LV Name                importlv
  VG Name                importvg
  LV UUID                2jPwL5-pP2N-kMEr-wwiS-Syu8-2krB-JmKwNI

# vgs  
  VG       #PV #LV #SN Attr   VSize   VFree
  systemvg   1  24   0 wz--n- 194.88g 4.88g
  tempvg     1   1   ...

# lvremove /dev/tempvg/tempvg_importlv

# vgs
  VG       #PV #LV #SN Attr   VSize   VFree
  systemvg   1  24   0 wz--n- 194.88g 4.88g
  tempvg     1   0   ...

# pvs
  PV         VG       Fmt  Attr PSize   PFree
  /dev/sdb   systemvg lvm2 a--  205.88g     0
  /dev/sdc   importvg lvm2 a--  100.00g 100.00g

# echo 1 > /sys/block/sdc/device/delete

# pvs
  PV         VG       Fmt  Attr PSize   PFree
  /dev/sdb   systemvg lvm2 a--  205.88g     0 
=====================================================





EXAMPLE 4


Prosze o dodanie filesystemu /tsms4roz_archfailover na lx11fc.
Filesystem ma być mirrorowany na poziomie lvm'a.



sudo /usr/sbin/pvs

# fdisk -l

# pvcreate /dev/sdXXX
# pvs
# pvdisplay


/dev/mapper/s4rozarchvg-s4rozarchlv
                      384G  313G   72G  82% /tsms4roz_arch
/tsms4roz_archfailover

s4rozarchfailovervg
s4rozarchfailoverlv

# vgcreate s4rozarchfailovervg /dev/sdXXX
# vgs
# vgdisplay

# lvcreate -L512M -n s4rozarchfailoverlv s4rozarchfailovervg
# lvs
# lvs s4rozarchfailovervg
# lvdisplay

# mkfs.ext3 /dev/mapper/s4rozarchfailovervg-s4rozarchfailoverlv

# mkdir /tsms4roz_archfailover

# vi /etc/fstab
/dev/mapper/s4rozarchfailovervg-s4rozarchfailoverlv    /tsms4roz_archfailover    ext3    defaults    0 0


# mount -a


---------------------------------------------------------------
multipath -r
multipath -ll

echo "- - -" > /sys/class/scsi_host/host0/scan
echo "- - -" > /sys/class/scsi_host/host1/scan
echo "- - -" > /sys/class/scsi_host/host2/scan


60:06:01:60:1A:20:29:00:A6:8C:47:6A:7D:5E:E3:11    


vgcreate s4rozarchfailovervg /dev/mpath/3$(echo "60:06:01:60:1A:20:29:00:A6:8C:47:6A:7D:5E:E3:11" | sed 's/://g' | tr '[A-Z]' '[a-z]')  /dev/mpath/3$(echo "60:06:01:60:1A:20:29:00:94:EE:FD:3A:BD:60:E3:11" | sed 's/://g' | tr '[A-Z]' '[a-z]') /dev/mpath/3$(echo "60:06:01:60:1E:20:29:00:AE:7A:70:DC:7D:5E:E3:11" | sed 's/://g' | tr '[A-Z]' '[a-z]') /dev/mpath/3$(echo "60:06:01:60:1E:20:29:00:90:53:17:F5:C4:60:E3:11" | sed 's/://g' | tr '[A-Z]' '[a-z]')





 /dev/mpath/3$(echo "" | sed 's/://g' | tr '[A-Z]' '[a-z]')


mkfs.ext3 /dev/s4rozarchfailovervg/s4rozarchfailoverlv



sudo /usr/sbin/pvs ...


EXAMPLE 5



Utworzenie fs:
/backup
/backup2
obydwa po 400GB

1. VMWare - dodanie HDD 800GB

2. RHEL:
pvcreate /dev/sdc
vgcreate backupvg /dev/sdc
lvcreate -L400G -n backup backupvg
lvcreate -L400G -n backup2 backupvg
lvcreate -l+100%FREE -n backup2 backupvg
mkfs.ext4 /dev/mapper/backupvg-backup
mkfs.ext4 /dev/mapper/backupvg-backup2
mkdir /backup
mkdir /backup2
vi /etc/fstab
mount -a
chown ondba:gondba /backup
chown ondba:gondba /backup2

=============================================

# ls /sys/class/scsi_host
host0 host1 host2
# echo "- - -" > /sys/class/scsi_host/host0/scan
# echo "- - -" > /sys/class/scsi_host/host1/scan
# echo "- - -" > /sys/class/scsi_host/host2/scan


=============================================

 

EXAMPLE 6


Proszę o zwiększenie fs'u
/cdw/logarch na lx13he o 5TB, do wartości 6,5 TB.
Osoby kontaktowe po stronie BSK+
Jakub Czerwonko, Dariusz Ozga.

Uwaga: Wskazane jest uzyskanie takiej samej wartości jak zostanie zrobiona dla takiego fs'u na lx1437 w ramach ZP535294

/dev/mapper/cdwvgp-logarchlv           1.5T  193M  1.4T   1% /cdw/logarch
/dev/mapper/cdwvgp-logarchlv

VNX_11_OW_840 : LUN 30
60:06:01:60:40:C0:31:00:48:B1:C4:83:41:DB:E3:11

VNX_11_OW_840 : LUN 31
60:06:01:60:40:C0:31:00:4C:B1:C4:83:41:DB:E3:11

VNX_11_OW_840 : LUN 32
60:06:01:60:40:C0:31:00:C8:DC:17:8F:41:DB:E3:11

6006016040c0310048b1c48341dbe311
6006016040c031004cb1c48341dbe311
6006016040c03100c8dc178f41dbe311

ls -l /sys/class/scsi_host
echo "- - -" > /sys/class/scsi_host/host0/scan
echo "- - -" > /sys/class/scsi_host/host1/scan
itd.

multipath -ll | grep -A11 6006016040c0310048b1c48341dbe311
multipath -ll | grep -A11 6006016040c031004cb1c48341dbe311
multipath -ll | grep -A11 6006016040c03100c8dc178f41dbe311

multipath -ll | grep -A2 6006016040c0310048b1c48341dbe311
mpathbr (36006016040c0310048b1c48341dbe311) dm-124 DGC,VRAID
size=2.0T features='1 queue_if_no_path' hwhandler='1 alua' wp=r

multipath -ll | grep -A2 6006016040c031004cb1c48341dbe311
mpathbs (36006016040c031004cb1c48341dbe311) dm-125 DGC,VRAID
size=2.0T features='1 queue_if_no_path' hwhandler='1 alua' wp=rw

multipath -ll | grep -A2 6006016040c03100c8dc178f41dbe311
mpathbq (36006016040c03100c8dc178f41dbe311) dm-123 DGC,VRAID
size=1.0T features='1 queue_if_no_path' hwhandler='1 alua' wp=rw

pvcreate /dev/mapper/mpathbr
pvcreate /dev/mapper/mpathbs
pvcreate /dev/mapper/mpathbq

vgextend cdwvgp /dev/mapper/mpathbr
vgextend cdwvgp /dev/mapper/mpathbs
vgextend cdwvgp /dev/mapper/mpathbq

lvresize -l+100%FREE /dev/mapper/cdwvgp-logarchlv
resize2fs /dev/mapper/cdwvgp-logarchlv

takeover

prompt

kerberos

cisco jabber

meduza

zkw













